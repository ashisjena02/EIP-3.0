Name: Ashis Kumar Jena

Batch Number 1





**Activation Function:**

Activation function helps segregate useful data from irrelevant and least useful data.  In Neural network Activation functions decides if a neuron should be activated or not. It decides whether the input information is useful or if it can be ignored as it is less useful. Activation function is the non-linear transformation performed on input signals.  This Transformed output signals are provided as input to next level neurons. 

 

Activation functions are helps to learn and perform complex tasks. t is possible to perform back propagation with activation functions as the gradients are supplied along with the error to update the weights and biases. Sigmoid, Tanh, ReLu (Rectified Linier Unit) and softmax are few examples of Activation functions.

 

 

**Receptive field**

Receptive field can be explained as a number of pixels in an input image contributing to feature map. Receptive field in an input image would the pixel area that is actively focused on. Not all the pixels in a receptive field of a given feature are equally important.IN calculating the output feature, closer the pixel to the center of the field more is its contribution to the output field that means in a receptive field a feature mainly focuses on the middle of the region.

 

**Convolution:**

Convolution is a Kernel.  Kernel slides around the images and commutes the sum of pixels each multiplied by the weight of corresponding kernel and bias is also added. To convolve means to “roll together”. In mathematical terms convolution is measuring how much two functions overlap as one passes another. 

It is a method to extract features from an image. It is a mathematical operation just like addition or subtraction. Convolution is used for Signal filtration, signal correlation among each other and finding signal patterns. In order to keep the output size same as the input size signals are padded with zeros

 

 

 

 

 

 

 

**EPOCH:**

Epoch is a hyper parameter used for learning algorithms which defines the number of times training dataset will work through training dataset. EPOC consists of one or more batches. AN EPOC with one batch is called batch gradient descent learning algorithm. The number of Epochs is usually large which allows the algorithm to run till it minimizes the errors from the model.

Number of epochs is usually the number of complete passes through the training dataset, number of Epochs can be set to an integer value between one and infinity. One Epoch is usually one complete training cycle on training set. Number of batches needed to complete one epoch is called Iteration

 

**Feature Map** 

Feature Map often known as Activation Map is used in convolution neural networks to extract important features and store it in a map, wherever the features are found those are added to the feature maps. It's called feature map because it is in a way searching for where a particular feature appears in the input image and adds it to the map. And since it tells exact location of a particular

 Feature in an image it's therefore a "map" for an example convolution network looks for features like nose or teeth and adds it to the map whenever and wherever it finds nose. Each feature map is meant to look for different feature than the previous one. So, one feature map will store location of nose, while other feature map would store teeth.

 